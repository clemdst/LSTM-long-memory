{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "08ee5ef4",
      "metadata": {
        "id": "08ee5ef4"
      },
      "source": [
        "In this notebook, we prove that LSTM fail into capturing long memory in multivariate time series.\n",
        "\n",
        "Then, we applied the two complementary tests detailed in the paper *A Statistical Investigation of Long Memory in Language and Music* by Greaves-Tunnell, Alexander and Harchaoui, Zaid.\n",
        "\n",
        "The two tests consists into checking the GSE statistics of the long memory vector d in the last hidden layer of the trained LSTM.\n",
        "\n",
        "The difference between the two tests: the first test consists into training the LSTM on a Fractionnaly differenced WN while the second test consists into training the LSTM on a WN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4MdKXWJasms0",
      "metadata": {
        "id": "4MdKXWJasms0"
      },
      "outputs": [],
      "source": [
        "#Google collab setting\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"/content/LSTM-long-memory/Test Kelly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6f3a6a84",
      "metadata": {
        "id": "6f3a6a84"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from _varfima import sim_VARFIMA, sim_FD\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "#import optuna\n",
        "torch.manual_seed(42) #For reproductibility\n",
        "from d_test import compute_total_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cd5587d",
      "metadata": {
        "id": "8cd5587d"
      },
      "source": [
        "##  Building of a LSTM with two layers for multivariate time series prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0bd2c202",
      "metadata": {
        "id": "0bd2c202"
      },
      "outputs": [],
      "source": [
        "#Let's build the LSTM model for time series prediction\n",
        "\n",
        "class LSTMPredictor(nn.Module):\n",
        "    \"\"\"LSTM for time series prediction\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=2,\n",
        "                 dropout=0.2, forecast_horizon=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size: Number of features (k variables)\n",
        "            hidden_size: Size of the hidden state\n",
        "            num_layers: Number of layers in the LSTM\n",
        "            dropout: Dropout rate between LSTM layers\n",
        "            forecast_horizon: Number of time steps to predict\n",
        "        \"\"\"\n",
        "        super(LSTMPredictor, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "\n",
        "        # Couche LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True  # (batch, seq, feature)\n",
        "        )\n",
        "\n",
        "        # Couche fully connected pour la prédiction\n",
        "        self.fc = nn.Linear(hidden_size, input_size * forecast_horizon)\n",
        "\n",
        "        self.input_size = input_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_length, input_size)\n",
        "        Returns:\n",
        "            predictions: (batch, forecast_horizon, input_size)\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # LSTM forward\n",
        "        # lstm_out: (batch, seq_length, hidden_size)\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "\n",
        "        # Prendre la dernière sortie temporelle\n",
        "        last_output = lstm_out[:, -1, :]  # (batch, hidden_size)\n",
        "\n",
        "        # Prédiction\n",
        "        predictions = self.fc(last_output)  # (batch, input_size*forecast_horizon)\n",
        "\n",
        "        # Reshape pour séparer forecast_horizon et input_size\n",
        "        predictions = predictions.view(batch_size, self.forecast_horizon,\n",
        "                                      self.input_size)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "k=200"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92b4676a",
      "metadata": {},
      "source": [
        "## Test 1: Integration of Fractionnaly Differenced WN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33e8a2fa",
      "metadata": {},
      "source": [
        "##### Generation of a random Fractionally Differenced White Noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "90745560",
      "metadata": {},
      "outputs": [],
      "source": [
        "# First let's generate a random long memory paramater vector (d)\n",
        "np.random.seed(42)\n",
        "k= 200 #number of time series\n",
        "d_min, d_max = 0.05, 0.45  # to have a long memory vector parameter (d in ]0,0.5[**p)\n",
        "d = torch.tensor(np.random.uniform(d_min, d_max, size=k), dtype=torch.float32)\n",
        "\n",
        "#Then let's generated a fractionnally differenced white noise based on the generated d vector, of length T\n",
        "T=2**16 #same length as in the paper, and same number k of time series\n",
        "FD_seq, _ = sim_FD(T=2**16, k=200, d=d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95184d3c",
      "metadata": {
        "id": "95184d3c"
      },
      "source": [
        "##### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "afcb7a56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afcb7a56",
        "outputId": "b87319bb-327a-499b-c7dd-2ec041690c2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([200, 65536])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FD_seq.shape #we have a non supervised dataset of shape (k,T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d96cff",
      "metadata": {
        "id": "f7d96cff"
      },
      "outputs": [],
      "source": [
        "def unfold_sequence_to_supervised_dataset(data, seq_length, forecast_horizon=1):\n",
        "    \"\"\"\n",
        "    Create a supervised dataset from multivariate time series sequence.\n",
        "    with a rolling window approach.\n",
        "    Args:\n",
        "        data: torch.Tensor of shape (k, T)\n",
        "        seq_length: length of the input window (size of X)\n",
        "        forecast_horizon: number of steps to predict (size of y)\n",
        "\n",
        "    Returns:\n",
        "        X: (n_samples, seq_length, k)\n",
        "        y: (n_samples, forecast_horizon, k)\n",
        "    \"\"\"\n",
        "    data = data.T  # (T, k)\n",
        "    T, k = data.shape\n",
        "\n",
        "    n_samples = T - seq_length - forecast_horizon + 1\n",
        "\n",
        "    X = torch.zeros((n_samples, seq_length, k))\n",
        "    y = torch.zeros((n_samples, forecast_horizon, k))\n",
        "\n",
        "    for idx in range(n_samples):\n",
        "        X[idx] = data[idx:idx+seq_length]\n",
        "        y[idx] = data[idx+seq_length:idx+seq_length+forecast_horizon]\n",
        "\n",
        "    return X.float(), y.float()\n",
        "\n",
        "dataset = unfold_sequence_to_supervised_dataset(FD_seq, seq_length=512, forecast_horizon=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d2640ee",
      "metadata": {},
      "source": [
        "We transform the time serie into a supervised learning dataset with a rolling window process, and a length equal to seq_length=512 for the X, so the long memory can be visible on the different windows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "36ca3571",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36ca3571",
        "outputId": "a8c3df22-6b66-434c-ee36-67c2ca47d07b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of X: torch.Size([65526, 10, 200]) \n",
            "Size of y: torch.Size([65526, 1, 200])\n"
          ]
        }
      ],
      "source": [
        "print(f\"Size of X: {dataset[0].shape} \\nSize of y: {dataset[1].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a5cee7a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5cee7a9",
        "outputId": "89ed516a-9007-4219-eaf7-6c829c47e0c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Prepared dataset:\n",
            "  - Train samples: 52420\n",
            "  - Val samples: 13106\n",
            "  - Batch size: 32\n"
          ]
        }
      ],
      "source": [
        "#Let's prepare the dataset of training and validation\n",
        "\n",
        "#tensorisation of the dataset\n",
        "full_dataset= torch.utils.data.TensorDataset(dataset[0], dataset[1])\n",
        "\n",
        "# Split train/validation\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    full_dataset, [train_size, val_size]\n",
        ")\n",
        "\n",
        "# Batching\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\" Prepared dataset:\")\n",
        "print(f\"  - Train samples: {len(train_dataset)}\")\n",
        "print(f\"  - Val samples: {len(val_dataset)}\")\n",
        "print(f\"  - Batch size: {batch_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a20dd8a",
      "metadata": {
        "id": "1a20dd8a"
      },
      "source": [
        "##### Data Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cca3086",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8cca3086",
        "outputId": "e31a9e2f-c578-4c16-b7c4-11942abff078"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m model = lstm_model.to(device)\n\u001b[32m     16\u001b[39m criterion = nn.MSELoss()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m optimizer = \u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m5\u001b[39m, eps=\u001b[32m1e-6\u001b[39m) \u001b[38;5;66;03m#automatic learning rate adjustment\u001b[39;00m\n\u001b[32m     20\u001b[39m num_epochs = \u001b[32m150\u001b[39m \u001b[38;5;66;03m#iteration number\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:101\u001b[39m, in \u001b[36mAdam.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor betas[1] must be 1-element\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m defaults = {\n\u001b[32m     89\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: lr,\n\u001b[32m     90\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m: betas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdecoupled_weight_decay\u001b[39m\u001b[33m\"\u001b[39m: decoupled_weight_decay,\n\u001b[32m    100\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:401\u001b[39m, in \u001b[36mOptimizer.__init__\u001b[39m\u001b[34m(self, params, defaults)\u001b[39m\n\u001b[32m    398\u001b[39m     param_groups = [{\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: param_groups}]\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._warned_capturable_if_run_uncaptured = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_compile.py:46\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m disable_fn = \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[33m\"\u001b[39m\u001b[33m__dynamo_disable\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# We can safely turn off functools.wraps here because the inner\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# already wraps fn in the outer scope.\u001b[39;00m\n\u001b[32m     50\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     aot_compile,\n\u001b[32m     15\u001b[39m     config,\n\u001b[32m     16\u001b[39m     convert_frame,\n\u001b[32m     17\u001b[39m     eval_frame,\n\u001b[32m     18\u001b[39m     functional_export,\n\u001b[32m     19\u001b[39m     resume_execution,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_dynamo\\aot_compile.py:15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprecompile_context\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrecompileContext\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_frame\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Hooks\n\u001b[32m     19\u001b[39m log = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:57\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CallbackTrigger\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:53\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ObservedException, TensorifyScalarRestartAnalysis\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstructured\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dump_file\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_dynamo\\exc.py:45\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_file_path_2\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_dynamo\\utils.py:67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytree\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fx\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m S\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BoolLike, FloatLike, IntLike\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\sympy\\__init__.py:77\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (to_cnf, to_dnf, to_nnf, And, Or, Not, Xor, Nand, Nor,\n\u001b[32m     71\u001b[39m         Implies, Equivalent, ITE, POSform, SOPform, simplify_logic, bool_map,\n\u001b[32m     72\u001b[39m         true, false, satisfiable)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01massumptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (AppliedPredicate, Predicate, AssumptionsContext,\n\u001b[32m     75\u001b[39m         assuming, Q, ask, register_handler, remove_handler, refine)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n\u001b[32m     78\u001b[39m         degree, total_degree, degree_list, LC, LM, LT, pdiv, prem, pquo,\n\u001b[32m     79\u001b[39m         pexquo, div, rem, quo, exquo, half_gcdex, gcdex, invert,\n\u001b[32m     80\u001b[39m         subresultants, resultant, discriminant, cofactors, gcd_list, gcd,\n\u001b[32m     81\u001b[39m         lcm_list, lcm, terms_gcd, trunc, monic, content, primitive, compose,\n\u001b[32m     82\u001b[39m         decompose, sturm, gff_list, gff, sqf_norm, sqf_part, sqf_list, sqf,\n\u001b[32m     83\u001b[39m         factor_list, factor, intervals, refine_root, count_roots, all_roots,\n\u001b[32m     84\u001b[39m         real_roots, nroots, ground_roots, nth_power_roots_poly, cancel,\n\u001b[32m     85\u001b[39m         reduced, groebner, is_zero_dimensional, GroebnerBasis, poly,\n\u001b[32m     86\u001b[39m         symmetrize, horner, interpolate, rational_interpolate, viete, together,\n\u001b[32m     87\u001b[39m         BasePolynomialError, ExactQuotientFailed, PolynomialDivisionFailed,\n\u001b[32m     88\u001b[39m         OperationNotSupported, HeuristicGCDFailed, HomomorphismFailed,\n\u001b[32m     89\u001b[39m         IsomorphismFailed, ExtraneousFactors, EvaluationFailed,\n\u001b[32m     90\u001b[39m         RefinementFailed, CoercionFailed, NotInvertible, NotReversible,\n\u001b[32m     91\u001b[39m         NotAlgebraic, DomainError, PolynomialError, UnificationFailed,\n\u001b[32m     92\u001b[39m         GeneratorsError, GeneratorsNeeded, ComputationFailed,\n\u001b[32m     93\u001b[39m         UnivariatePolynomialError, MultivariatePolynomialError,\n\u001b[32m     94\u001b[39m         PolificationFailed, OptionError, FlagError, minpoly,\n\u001b[32m     95\u001b[39m         minimal_polynomial, primitive_element, field_isomorphism,\n\u001b[32m     96\u001b[39m         to_number_field, isolate, round_two, prime_decomp, prime_valuation,\n\u001b[32m     97\u001b[39m         galois_group, itermonomials, Monomial, lex, grlex,\n\u001b[32m     98\u001b[39m         grevlex, ilex, igrlex, igrevlex, CRootOf, rootof, RootOf,\n\u001b[32m     99\u001b[39m         ComplexRootOf, RootSum, roots, Domain, FiniteField, IntegerRing,\n\u001b[32m    100\u001b[39m         RationalField, RealField, ComplexField, PythonFiniteField,\n\u001b[32m    101\u001b[39m         GMPYFiniteField, PythonIntegerRing, GMPYIntegerRing, PythonRational,\n\u001b[32m    102\u001b[39m         GMPYRationalField, AlgebraicField, PolynomialRing, FractionField,\n\u001b[32m    103\u001b[39m         ExpressionDomain, FF_python, FF_gmpy, ZZ_python, ZZ_gmpy, QQ_python,\n\u001b[32m    104\u001b[39m         QQ_gmpy, GF, FF, ZZ, QQ, ZZ_I, QQ_I, RR, CC, EX, EXRAW,\n\u001b[32m    105\u001b[39m         construct_domain, swinnerton_dyer_poly, cyclotomic_poly,\n\u001b[32m    106\u001b[39m         symmetric_poly, random_poly, interpolating_poly, jacobi_poly,\n\u001b[32m    107\u001b[39m         chebyshevt_poly, chebyshevu_poly, hermite_poly, hermite_prob_poly,\n\u001b[32m    108\u001b[39m         legendre_poly, laguerre_poly, apart, apart_list, assemble_partfrac_list,\n\u001b[32m    109\u001b[39m         Options, ring, xring, vring, sring, field, xfield, vfield, sfield)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseries\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (Order, O, limit, Limit, gruntz, series, approximants,\n\u001b[32m    112\u001b[39m         residue, EmptySequence, SeqPer, SeqFormula, sequence, SeqAdd, SeqMul,\n\u001b[32m    113\u001b[39m         fourier_series, fps, difference_delta, limit_seq)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (factorial, factorial2, rf, ff, binomial,\n\u001b[32m    116\u001b[39m         RisingFactorial, FallingFactorial, subfactorial, carmichael,\n\u001b[32m    117\u001b[39m         fibonacci, lucas, motzkin, tribonacci, harmonic, bernoulli, bell, euler,\n\u001b[32m   (...)\u001b[39m\u001b[32m    138\u001b[39m         Znm, elliptic_k, elliptic_f, elliptic_e, elliptic_pi, beta, mathieus,\n\u001b[32m    139\u001b[39m         mathieuc, mathieusprime, mathieucprime, riemann_xi, betainc, betainc_regularized)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\sympy\\polys\\__init__.py:79\u001b[39m\n\u001b[32m      3\u001b[39m __all__ = [\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPoly\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPurePoly\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpoly_from_expr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mparallel_poly_from_expr\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdegree\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtotal_degree\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdegree_list\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLC\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLM\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLT\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpdiv\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mprem\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpquo\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfield\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxfield\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvfield\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msfield\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     66\u001b[39m ]\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolytools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (Poly, PurePoly, poly_from_expr,\n\u001b[32m     69\u001b[39m         parallel_poly_from_expr, degree, total_degree, degree_list, LC, LM,\n\u001b[32m     70\u001b[39m         LT, pdiv, prem, pquo, pexquo, div, rem, quo, exquo, half_gcdex, gcdex,\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         nth_power_roots_poly, cancel, reduced, groebner, is_zero_dimensional,\n\u001b[32m     77\u001b[39m         GroebnerBasis, poly)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolyfuncs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (symmetrize, horner, interpolate,\n\u001b[32m     80\u001b[39m         rational_interpolate, viete)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrationaltools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m together\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolyerrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (BasePolynomialError, ExactQuotientFailed,\n\u001b[32m     85\u001b[39m         PolynomialDivisionFailed, OperationNotSupported, HeuristicGCDFailed,\n\u001b[32m     86\u001b[39m         HomomorphismFailed, IsomorphismFailed, ExtraneousFactors,\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m         MultivariatePolynomialError, PolificationFailed, OptionError,\n\u001b[32m     92\u001b[39m         FlagError)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\sympy\\polys\\polyfuncs.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolys\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolyoptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m allowed_flags, build_options\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolys\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolytools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m poly_from_expr, Poly\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolys\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecialpolys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     symmetric_poly, interpolating_poly)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolys\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sring\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutilities\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numbered_symbols, take, public\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\sympy\\polys\\specialpolys.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontainers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msingleton\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m S\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mntheory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nextprime\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolys\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdensearith\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     dmp_add_term, dmp_neg, dmp_mul, dmp_sqr\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolys\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdensebasic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     dmp_zero, dmp_one, dmp_ground,\n\u001b[32m     13\u001b[39m     dup_from_raw_dict, dmp_raise, dup_random\n\u001b[32m     14\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\sympy\\ntheory\\__init__.py:16\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprimetest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m isprime, is_gaussian_prime, is_mersenne_prime\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfactor_\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m divisors, proper_divisors, factorint, multiplicity, \\\n\u001b[32m      9\u001b[39m     multiplicity_in_factorial, perfect_power, factor_cache, pollard_pm1, \\\n\u001b[32m     10\u001b[39m     pollard_rho, primefactors, totient, \\\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     is_perfect, is_abundant, is_deficient, is_amicable, is_carmichael, \\\n\u001b[32m     14\u001b[39m     abundance, dra, drm\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartitions_\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m npartitions\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresidue_ntheory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_primitive_root, is_quad_residue, \\\n\u001b[32m     18\u001b[39m     legendre_symbol, jacobi_symbol, n_order, sqrt_mod, quadratic_residues, \\\n\u001b[32m     19\u001b[39m     primitive_root, nthroot_mod, is_nthpow_residue, sqrt_mod_iter, mobius, \\\n\u001b[32m     20\u001b[39m     discrete_log, quadratic_congruence, polynomial_congruence\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmultinomial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m binomial_coefficients, binomial_coefficients_list, \\\n\u001b[32m     22\u001b[39m     multinomial_coefficients\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1022\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1118\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1217\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "#Let's train the LSTM on the generated data using a ADAM optimizer and MSE loss\n",
        "#parameters of training:\n",
        "#for the optimizer: lr=0.01, scheduler (LROnPlateau): patience=5, eps=1e-6\n",
        "#for the LSTM: 64 hidden size, 2 layers, dropout of 0.2\n",
        "\n",
        "#Let's load the built LSTM model of two layers adapted to our multivariate time series of size k=200\n",
        "lstm_model = LSTMPredictor(\n",
        "    input_size=k,\n",
        "    hidden_size=64,\n",
        "    dropout=0.2)\n",
        "\n",
        "# Configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = lstm_model.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, eps=1e-6) #automatic learning rate adjustment\n",
        "\n",
        "num_epochs = 150 #iteration number\n",
        "\n",
        "# Historique des pertes\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(f\"Entraînement sur {device}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # === TRAINING ===\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(X_batch)\n",
        "        loss = criterion(predictions, y_batch)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # === VALIDATION ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            predictions = model(X_batch)\n",
        "            loss = criterion(predictions, y_batch)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Affichage\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "              f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"✓ Entraînement terminé\")\n",
        "\n",
        "# Visualisation of loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.title('Training History')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Let's save the trained model\n",
        "torch.save({\n",
        "    'epoch': num_epochs,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'train_loss': train_losses[-1],\n",
        "    'val_loss': val_losses[-1],\n",
        "}, 'lstm_on_fractionnaly_differenciated_WN.pth')\n",
        "\n",
        "print(\"✓ Modèle sauvegardé: lstm_on_fractionnaly_differenciated_WN.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pWdnSHhVCqvx",
      "metadata": {
        "id": "pWdnSHhVCqvx"
      },
      "source": [
        "The validation loss seems to have reached a plateau at around 50 iterations with a loss around 1.12. We can stop our training at 50 iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2405fe1a",
      "metadata": {
        "id": "2405fe1a"
      },
      "source": [
        "#### Let's verify the long memory property in the LSTM trained"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xEg-OWBSK_Gj",
      "metadata": {
        "id": "xEg-OWBSK_Gj"
      },
      "source": [
        "First, let's extract the last layer of our trained LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DJOspbk-AdY3",
      "metadata": {
        "id": "DJOspbk-AdY3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hidden representation shape: (13106, 64)\n",
            "Number of time steps in test: 13106\n",
            "Hidden state dimension: 64\n"
          ]
        }
      ],
      "source": [
        "#Loading the trained model\n",
        "model.load_state_dict(torch.load('lstm_on_fractionnaly_differenciated_WN.pth')[\"model_state_dict\"])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "hidden_states_val = []\n",
        "with torch.no_grad():\n",
        "    for X_batch, _ in val_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        lstm_out, (h_n, c_n) = model.lstm(X_batch)\n",
        "        hidden_states_val.append(h_n[-1].cpu()) \n",
        "\n",
        "hidden_rep = torch.cat(hidden_states_val, dim=0).numpy()  \n",
        "\n",
        "print(f\"Hidden representation shape: {hidden_rep.shape}\")\n",
        "print(f\"Number of time steps in test: {hidden_rep.shape[0]}\")\n",
        "print(f\"Hidden state dimension: {hidden_rep.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "117f8191",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Theoretical Framework: VARFIMA Behavior in LSTMs\n",
        "\n",
        "If an LSTM successfully captures long memory, it should behave like a **VARFIMA model**. Let's examine what this means:\n",
        "\n",
        "### VARFIMA Model\n",
        "A VARFIMA process is defined as:\n",
        "$$\\Psi(B) X_t = Z_t$$\n",
        "\n",
        "where the inverse filter is:\n",
        "$$\\Psi(B) = (1-B)^d$$\n",
        "\n",
        "and the inverse operation is:\n",
        "$$X_t = \\Psi(B)^{-1} Z_t = (1-B)^{-d} Z_t$$\n",
        "\n",
        "### Test 1: Fractionally Differenced Input\n",
        "\n",
        "Our genereated input is a **fractionally differenced white noise**:\n",
        "$$\\tilde{X}_t = (1-B)^d Z_t$$\n",
        "\n",
        "If the LSTM has learned the VARFIMA structure, its hidden state should perform the inverse operation:\n",
        "$$h_t = \\Psi(\\tilde{X}_t) = (1-B)^{-d} \\tilde{X}_t = (1-B)^{-d} (1-B)^d Z_t = Z_t$$\n",
        "\n",
        "**Expected result**: If the LSTM has long memory property, the hidden state $h_t$ should equal white noise $Z_t$, which has **$d = 0$**, meaning that the LSTM sucessfully captured and removed the long memory in the differenciated white noise.\n",
        "\n",
        "#### Hypothesis Test\n",
        "\n",
        "We test:\n",
        "- **H0**: $d = 0$ (hidden states have no long memory, LSTM has long memory)\n",
        "- **H1**: $d \\neq 0$ (hidden states have long memory, LSTM does not have long memory)\n",
        "\n",
        "The test uses the GSE statistics to estimate the long memory parameter $d$ from the hidden representation and returns a p-value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "_N4d4SBPHSUj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N4d4SBPHSUj",
        "outputId": "75602cab-406f-4f9c-c1c4-d0e84a1b0c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST 1: LSTM trained on Fractionally Differenced White Noise\n",
            "  Total long memory: 0.004710\n",
            "  Std deviation: 0.062500\n",
            "  p-value (H0: d=0): 4.6996e-01\n",
            "\n",
            "Conclusion:\n",
            "   H0 is accepted (p ≥ 0.05): d ≈ 0\n",
            " → Hidden states does not contain long memory anymore --> LSTM has long memory property\n"
          ]
        }
      ],
      "source": [
        "# Test long memory on LSTM hidden states\n",
        "\n",
        "hidden_rep_transposed = hidden_rep.T  \n",
        "\n",
        "tot_mem_hidden, std_var_hidden, p_val_hidden = compute_total_memory(\n",
        "    hidden_rep_transposed)\n",
        "\n",
        "\n",
        "print(f\"TEST 1: LSTM trained on Fractionally Differenced White Noise\")\n",
        "print(f\"  Total long memory: {tot_mem_hidden:.6f}\")\n",
        "print(f\"  Std deviation: {np.sqrt(std_var_hidden):.6f}\")\n",
        "print(f\"  p-value (H0: d=0): {p_val_hidden:.4e}\")\n",
        "print(f\"\\nConclusion:\")\n",
        "if p_val_hidden < 0.05:\n",
        "    print(f\"  ✗ H0 is rejected (p < 0.05): d ≠ 0\")\n",
        "    print(f\"  → Hidden states still contains long memory --> LSTM has no long memory property\")\n",
        "else:\n",
        "    print(f\"   H0 is accepted (p ≥ 0.05): d ≈ 0\")\n",
        "    print(f\" → Hidden states does not contain long memory anymore --> LSTM has long memory property\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3b2cc38",
      "metadata": {},
      "source": [
        "We ran the test on the last layer of our LSTM and the test gave a long memory statistic d = 0.0047 and a non significative p-value around 0.7\n",
        "\n",
        "We can't reject the hypothesis of absence of long memory. The trained LSTM failed to adapt the behaviour of a VARFIMA. It means that it does not have long memory property."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd1257a0",
      "metadata": {},
      "source": [
        "## Test 2: Long memory transformation of white noise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e11cd35e",
      "metadata": {},
      "source": [
        "### Generation of a White Noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8f17ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: Generate White Noise (no long memory)\n",
        "# This is the complementary test to Test 1\n",
        "# We train LSTM on WHITE NOISE (short memory) and check if hidden states remain short memory\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 2: LSTM trained on White Noise (NO long memory)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Generate pure White Noise (no long memory, just random Gaussian noise)\n",
        "np.random.seed(42)\n",
        "k_wn = 200\n",
        "T_wn = 2**16\n",
        "WN_seq = torch.randn(k_wn, T_wn)  # Pure white noise: N(0,1)\n",
        "\n",
        "print(f\"\\nGenerated White Noise:\")\n",
        "print(f\"  Shape: {WN_seq.shape}\")\n",
        "print(f\"  Mean: {WN_seq.mean():.6f} (should be ~0)\")\n",
        "print(f\"  Std: {WN_seq.std():.6f} (should be ~1)\")\n",
        "print(f\"  Min/Max: [{WN_seq.min():.4f}, {WN_seq.max():.4f}]\")\n",
        "\n",
        "# 2. Create rolling window dataset\n",
        "X_wn, y_wn = unfold_sequence_to_supervised_dataset(\n",
        "    WN_seq, seq_length=512, forecast_horizon=1\n",
        ")\n",
        "\n",
        "full_dataset_wn = TensorDataset(X_wn, y_wn)\n",
        "train_size_wn = int(0.8 * len(full_dataset_wn))\n",
        "val_size_wn = len(full_dataset_wn) - train_size_wn\n",
        "\n",
        "train_dataset_wn, val_dataset_wn = torch.utils.data.random_split(\n",
        "    full_dataset_wn, [train_size_wn, val_size_wn]\n",
        ")\n",
        "\n",
        "train_loader_wn = DataLoader(train_dataset_wn, batch_size=32, shuffle=True)\n",
        "val_loader_wn = DataLoader(val_dataset_wn, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"\\nDataset shapes:\")\n",
        "print(f\"  X shape: {X_wn.shape}\")\n",
        "print(f\"  y shape: {y_wn.shape}\")\n",
        "print(f\"  Train samples: {len(train_dataset_wn)}\")\n",
        "print(f\"  Val samples: {len(val_dataset_wn)}\")\n",
        "\n",
        "<VSCode.Cell id=\"#VSC-8971018c\" language=\"python\">\n",
        "# Monte Carlo Study: n=100 independent realizations\n",
        "# Training LSTM on Fractionally Differenced WN with FIXED d parameter\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MONTE CARLO EXPERIMENT: n=100 Independent FD_seq with Fixed d\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Generate a FIXED d parameter (same for all 100 realizations)\n",
        "np.random.seed(42)\n",
        "k_mc = 200\n",
        "d_min, d_max = 0.05, 0.45\n",
        "d_mc = torch.tensor(np.random.uniform(d_min, d_max, size=k_mc), dtype=torch.float32)\n",
        "print(f\"\\nFixed d parameter (mean): {d_mc.mean():.4f}\")\n",
        "print(f\"Fixed d parameter (min, max): ({d_mc.min():.4f}, {d_mc.max():.4f})\")\n",
        "\n",
        "# 2. Setup for Monte Carlo loop\n",
        "n_realizations = 100\n",
        "T_mc = 2**16\n",
        "seq_length_mc = 10\n",
        "num_epochs_mc = 50  # fewer epochs for speed\n",
        "results_mc = {\n",
        "    'p_values': [],\n",
        "    'test_statistics': [],\n",
        "    'std_devs': [],\n",
        "    'rejected': [],\n",
        "    'failed_training': []\n",
        "}\n",
        "\n",
        "print(f\"\\nSettings:\")\n",
        "print(f\"  Number of realizations: {n_realizations}\")\n",
        "print(f\"  Sequence length: {T_mc}\")\n",
        "print(f\"  Epochs per realization: {num_epochs_mc}\")\n",
        "print(f\"  Significance level: α = 0.05\")\n",
        "print(f\"\\n{'Iter':>4} | {'Loss':>7} | {'d_stat':>8} | {'p-value':>9} | {'Result':>10}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "# 3. Monte Carlo loop\n",
        "for iter_mc in range(n_realizations):\n",
        "    try:\n",
        "        # Generate a NEW random FD_seq with the SAME d parameter\n",
        "        FD_seq_mc, _ = sim_FD(T=T_mc, k=k_mc, d=d_mc)\n",
        "        \n",
        "        # Prepare dataset\n",
        "        X_mc, y_mc = unfold_sequence_to_supervised_dataset(\n",
        "            FD_seq_mc, seq_length=seq_length_mc, forecast_horizon=1\n",
        "        )\n",
        "        \n",
        "        full_dataset_mc = TensorDataset(X_mc, y_mc)\n",
        "        train_size_mc = int(0.8 * len(full_dataset_mc))\n",
        "        val_size_mc = len(full_dataset_mc) - train_size_mc\n",
        "        \n",
        "        train_dataset_mc, val_dataset_mc = torch.utils.data.random_split(\n",
        "            full_dataset_mc, [train_size_mc, val_size_mc]\n",
        "        )\n",
        "        \n",
        "        train_loader_mc = DataLoader(train_dataset_mc, batch_size=32, shuffle=True)\n",
        "        val_loader_mc = DataLoader(val_dataset_mc, batch_size=32, shuffle=False)\n",
        "        \n",
        "        # Create and train LSTM\n",
        "        model_mc = LSTMPredictor(\n",
        "            input_size=k_mc,\n",
        "            hidden_size=64,\n",
        "            num_layers=2,\n",
        "            dropout=0.2\n",
        "        ).to(device)\n",
        "        \n",
        "        optimizer_mc = optim.Adam(model_mc.parameters(), lr=0.01)\n",
        "        criterion_mc = nn.MSELoss()\n",
        "        scheduler_mc = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer_mc, 'min', patience=5, eps=1e-6\n",
        "        )\n",
        "        \n",
        "        # Quick training\n",
        "        final_val_loss = 0.0\n",
        "        for epoch in range(num_epochs_mc):\n",
        "            model_mc.train()\n",
        "            train_loss = 0.0\n",
        "            \n",
        "            for X_batch, y_batch in train_loader_mc:\n",
        "                X_batch = X_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "                \n",
        "                optimizer_mc.zero_grad()\n",
        "                predictions = model_mc(X_batch)\n",
        "                loss = criterion_mc(predictions, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer_mc.step()\n",
        "                train_loss += loss.item()\n",
        "            \n",
        "            train_loss /= len(train_loader_mc)\n",
        "            \n",
        "            model_mc.eval()\n",
        "            final_val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for X_batch, y_batch in val_loader_mc:\n",
        "                    X_batch = X_batch.to(device)\n",
        "                    y_batch = y_batch.to(device)\n",
        "                    predictions = model_mc(X_batch)\n",
        "                    loss = criterion_mc(predictions, y_batch)\n",
        "                    final_val_loss += loss.item()\n",
        "            \n",
        "            final_val_loss /= len(val_loader_mc)\n",
        "            scheduler_mc.step(final_val_loss)\n",
        "        \n",
        "        # Extract hidden states from trained model\n",
        "        hidden_states_mc = []\n",
        "        model_mc.eval()\n",
        "        with torch.no_grad():\n",
        "            for X_batch, _ in val_loader_mc:\n",
        "                X_batch = X_batch.to(device)\n",
        "                lstm_out, (h_n, c_n) = model_mc.lstm(X_batch)\n",
        "                hidden_states_mc.append(h_n[-1].cpu())\n",
        "        \n",
        "        hidden_rep_mc = torch.cat(hidden_states_mc, dim=0).numpy().T  # (hidden_size, n_samples)\n",
        "        \n",
        "        # Compute long memory test\n",
        "        tot_mem_mc, std_var_mc, p_val_mc = compute_total_memory(hidden_rep_mc)\n",
        "        \n",
        "        # Store results\n",
        "        results_mc['p_values'].append(p_val_mc)\n",
        "        results_mc['test_statistics'].append(tot_mem_mc)\n",
        "        results_mc['std_devs'].append(np.sqrt(std_var_mc))\n",
        "        results_mc['rejected'].append(p_val_mc < 0.05)\n",
        "        results_mc['failed_training'].append(False)\n",
        "        \n",
        "        # Print progress\n",
        "        result_str = \"REJECT H0\" if p_val_mc < 0.05 else \"ACCEPT H0\"\n",
        "        print(f\"{iter_mc+1:4d} | {final_val_loss:7.4f} | {tot_mem_mc:8.4f} | {p_val_mc:9.2e} | {result_str:>10}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        results_mc['failed_training'].append(True)\n",
        "        results_mc['p_values'].append(np.nan)\n",
        "        results_mc['test_statistics'].append(np.nan)\n",
        "        results_mc['std_devs'].append(np.nan)\n",
        "        results_mc['rejected'].append(np.nan)\n",
        "        print(f\"{iter_mc+1:4d} | TRAINING FAILED\")\n",
        "\n",
        "# 4. Summary statistics\n",
        "print(\"-\" * 55)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "n_success = sum([not x for x in results_mc['failed_training']])\n",
        "n_failed = sum(results_mc['failed_training'])\n",
        "n_rejected = sum([1 for x in results_mc['rejected'] if x == True])\n",
        "n_accepted = n_success - n_rejected\n",
        "\n",
        "print(f\"\\nTraining Summary:\")\n",
        "print(f\"  ✓ Successful trainings: {n_success}/{n_realizations}\")\n",
        "print(f\"  ✗ Failed trainings: {n_failed}/{n_realizations}\")\n",
        "\n",
        "if n_success > 0:\n",
        "    print(f\"\\nHypothesis Test Summary (H0: d=0):\")\n",
        "    print(f\"  Rejections (p < 0.05): {n_rejected}/{n_success} ({100*n_rejected/n_success:.1f}%)\")\n",
        "    print(f\"  Acceptances (p ≥ 0.05): {n_accepted}/{n_success} ({100*n_accepted/n_success:.1f}%)\")\n",
        "    \n",
        "    p_vals = [p for p in results_mc['p_values'] if not np.isnan(p)]\n",
        "    test_stats = [t for t in results_mc['test_statistics'] if not np.isnan(t)]\n",
        "    \n",
        "    print(f\"\\nTest Statistic Distribution:\")\n",
        "    print(f\"  Mean d_stat: {np.mean(test_stats):.6f}\")\n",
        "    print(f\"  Std d_stat: {np.std(test_stats):.6f}\")\n",
        "    print(f\"  Min d_stat: {np.min(test_stats):.6f}\")\n",
        "    print(f\"  Max d_stat: {np.max(test_stats):.6f}\")\n",
        "    \n",
        "    print(f\"\\np-value Distribution:\")\n",
        "    print(f\"  Mean p-value: {np.mean(p_vals):.4e}\")\n",
        "    print(f\"  Median p-value: {np.median(p_vals):.4e}\")\n",
        "    print(f\"  Min p-value: {np.min(p_vals):.4e}\")\n",
        "    print(f\"  Max p-value: {np.max(p_vals):.4e}\")\n",
        "    \n",
        "    print(f\"\\nInterpretation:\")\n",
        "    print(f\"  H0 (d=0) REJECTED in {n_rejected}/{n_success} cases ({100*n_rejected/n_success:.1f}%)\")\n",
        "    print(f\"  H1 (d≠0) ACCEPTED in {n_rejected}/{n_success} cases ({100*n_rejected/n_success:.1f}%)\")\n",
        "    print(f\"  \")\n",
        "    print(f\"  H0 (d=0) ACCEPTED in {n_accepted}/{n_success} cases ({100*n_accepted/n_success:.1f}%)\")\n",
        "    print(f\"  H1 (d≠0) REJECTED in {n_accepted}/{n_success} cases ({100*n_accepted/n_success:.1f}%)\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
