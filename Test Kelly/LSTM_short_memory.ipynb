{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ee5ef4",
   "metadata": {},
   "source": [
    "In this notebbok, we prove that LSTM fail into capturing long memory in multivariate time series. \n",
    "\n",
    "Then, we applied the two complementary tests detailed in the paper *A Statistical Investigation of Long Memory in Language and Music* by Greaves-Tunnell, Alexander and Harchaoui, Zaid.\n",
    "\n",
    "The two tests consists into checking the GSE statistics of the long memory vector d in the last hidden layer of the trained LSTM. \n",
    "\n",
    "The difference between the two tests: the first test consists into training the LSTM on a Fractionnaly differenced WN while the second test consists into training the LSTM on a WN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f3a6a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20b59162cf0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from _varfima import sim_VARFIMA, sim_FD\n",
    "\n",
    "torch.manual_seed(42) #For reproductibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7671b492",
   "metadata": {},
   "source": [
    "# Test 1: Integration of Fractionnaly Differenced WN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e08af",
   "metadata": {},
   "source": [
    "##### Generation of a random Fractionally Differenced White Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b82de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's generate a random long memory paramater vector (d)\n",
    "\n",
    "np.random.seed(42)\n",
    "k= 200 #number of time series \n",
    "d_min, d_max = 0.05, 0.45  # to have a long memory vector parameter (d in ]0,0.5[**p)\n",
    "d = torch.tensor(np.random.uniform(d_min, d_max, size=k), dtype=torch.float32)\n",
    "\n",
    "#Then let's generated a fractionnally differenced white noise based on the generated d vector, of length T\n",
    "T=2**16 #same length as in the paper\n",
    "FD_seq, _ = sim_FD(T=2**16, k=200, d=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5587d",
   "metadata": {},
   "source": [
    "### Let's build a LSTM with two layers for multivariate time series prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bd2c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's build the LSTM model for time series prediction\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "    \"\"\"LSTM for time series prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, \n",
    "                 dropout=0.2, forecast_horizon=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Number of features (k variables)\n",
    "            hidden_size: Size of the hidden state\n",
    "            num_layers: Number of layers in the LSTM\n",
    "            dropout: Dropout rate between LSTM layers\n",
    "            forecast_horizon: Number of time steps to predict\n",
    "        \"\"\"\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        \n",
    "        # Couche LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True  # (batch, seq, feature)\n",
    "        )\n",
    "        \n",
    "        # Couche fully connected pour la prédiction\n",
    "        self.fc = nn.Linear(hidden_size, input_size * forecast_horizon)\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_length, input_size)\n",
    "        Returns:\n",
    "            predictions: (batch, forecast_horizon, input_size)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # LSTM forward\n",
    "        # lstm_out: (batch, seq_length, hidden_size)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Prendre la dernière sortie temporelle\n",
    "        last_output = lstm_out[:, -1, :]  # (batch, hidden_size)\n",
    "        \n",
    "        # Prédiction\n",
    "        predictions = self.fc(last_output)  # (batch, input_size*forecast_horizon)\n",
    "        \n",
    "        # Reshape pour séparer forecast_horizon et input_size\n",
    "        predictions = predictions.view(batch_size, self.forecast_horizon, \n",
    "                                      self.input_size)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "k=200\n",
    "\n",
    "#Let's load the built LSTM model of two layers adapted to our multivariate time series of size k=200\n",
    "lstm_model = LSTMPredictor(\n",
    "    input_size=k,           \n",
    "    hidden_size=64,        \n",
    "    dropout=0.2,           \n",
    "    forecast_horizon=1) \n",
    "#test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95184d3c",
   "metadata": {},
   "source": [
    "##### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "afcb7a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 65536])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FD_seq.shape #we have a non supervised dataset of shape (k,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7d96cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfold_sequence_to_supervised_dataset(data, seq_length=50, forecast_horizon=1):\n",
    "    \"\"\"\n",
    "    Create a supervised dataset from multivariate time series sequence.\n",
    "    with a rolling window approach.\n",
    "    Args:\n",
    "        data: torch.Tensor of shape (k, T) \n",
    "        seq_length: length of the input window (size of X)\n",
    "        forecast_horizon: number of steps to predict (size of y)\n",
    "\n",
    "    Returns:\n",
    "        X: (n_samples, seq_length, k)\n",
    "        y: (n_samples, forecast_horizon, k)\n",
    "    \"\"\"\n",
    "    data = data.T  # (T, k)\n",
    "    T, k = data.shape\n",
    "    \n",
    "    n_samples = T - seq_length - forecast_horizon + 1\n",
    "    \n",
    "    X = torch.zeros((n_samples, seq_length, k))\n",
    "    y = torch.zeros((n_samples, forecast_horizon, k))\n",
    "    \n",
    "    for idx in range(n_samples):\n",
    "        X[idx] = data[idx:idx+seq_length]\n",
    "        y[idx] = data[idx+seq_length:idx+seq_length+forecast_horizon]\n",
    "\n",
    "    return X.float(), y.float()\n",
    "\n",
    "dataset = unfold_sequence_to_supervised_dataset(FD_seq, seq_length=50, forecast_horizon=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36ca3571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: torch.Size([65486, 50, 200]) \n",
      "Size of y: torch.Size([65486, 1, 200])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of X: {dataset[0].shape} \\nSize of y: {dataset[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cee7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Prepared dataset:\n",
      "  - Train samples: 52388\n",
      "  - Val samples: 13098\n",
      "  - Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "#Let's prepare the dataset of training and validation\n",
    "\n",
    "#tensorisation of the dataset\n",
    "full_dataset= torch.utils.data.TensorDataset(dataset[0], dataset[1])\n",
    "\n",
    "# Split train/validation \n",
    "train_size = int(0.8 * len(full_dataset)) \n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Batching\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\" Prepared dataset:\")\n",
    "print(f\"  - Train samples: {len(train_dataset)}\")\n",
    "print(f\"  - Val samples: {len(val_dataset)}\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a20dd8a",
   "metadata": {},
   "source": [
    "##### Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8cca3086",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sympy' has no attribute 'core'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m model = lstm_model.to(device)\n\u001b[32m      7\u001b[39m criterion = nn.MSELoss()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m optimizer = \u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m5\u001b[39m, eps=\u001b[32m1e-6\u001b[39m) \u001b[38;5;66;03m#automatic learning rate adjustment\u001b[39;00m\n\u001b[32m     11\u001b[39m num_epochs = \u001b[32m50\u001b[39m \u001b[38;5;66;03m#50 iterations\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:101\u001b[39m, in \u001b[36mAdam.__init__\u001b[39m\u001b[34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor betas[1] must be 1-element\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m defaults = {\n\u001b[32m     89\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: lr,\n\u001b[32m     90\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m: betas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdecoupled_weight_decay\u001b[39m\u001b[33m\"\u001b[39m: decoupled_weight_decay,\n\u001b[32m    100\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:401\u001b[39m, in \u001b[36mOptimizer.__init__\u001b[39m\u001b[34m(self, params, defaults)\u001b[39m\n\u001b[32m    398\u001b[39m     param_groups = [{\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m: param_groups}]\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m._warned_capturable_if_run_uncaptured = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_compile.py:46\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m disable_fn = \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[33m\"\u001b[39m\u001b[33m__dynamo_disable\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# We can safely turn off functools.wraps here because the inner\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# already wraps fn in the outer scope.\u001b[39;00m\n\u001b[32m     50\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive, wrapping=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mTorchDynamo is a Python-level JIT compiler designed to make unmodified PyTorch programs faster.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mTorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03mseamlessly optimize PyTorch programs, including those using modern Python features.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     aot_compile,\n\u001b[32m     15\u001b[39m     config,\n\u001b[32m     16\u001b[39m     convert_frame,\n\u001b[32m     17\u001b[39m     eval_frame,\n\u001b[32m     18\u001b[39m     functional_export,\n\u001b[32m     19\u001b[39m     resume_execution,\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_dynamo\\aot_compile.py:15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprecompile_context\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrecompileContext\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_frame\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Hooks\n\u001b[32m     19\u001b[39m log = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:57\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CallbackTrigger\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compile_pg\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_convert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorifyState\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_context, CompileContext, CompileId, tracing\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structured\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:53\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dynamo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ObservedException, TensorifyScalarRestartAnalysis\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_guards\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracing, TracingContext\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstructured\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dump_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_dynamo\\exc.py:45\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_file_path_2\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m counters\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\_dynamo\\utils.py:67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_functorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytree\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fx\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:76\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ordered_set\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OrderedSet\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_python_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     77\u001b[39m     Application,\n\u001b[32m     78\u001b[39m     CeilToInt,\n\u001b[32m     79\u001b[39m     CleanDiv,\n\u001b[32m     80\u001b[39m     FloorDiv,\n\u001b[32m     81\u001b[39m     FloorToInt,\n\u001b[32m     82\u001b[39m     IntTrueDiv,\n\u001b[32m     83\u001b[39m     IsNonOverlappingAndDenseIndicator,\n\u001b[32m     84\u001b[39m     Max,\n\u001b[32m     85\u001b[39m     Min,\n\u001b[32m     86\u001b[39m     Mod,\n\u001b[32m     87\u001b[39m     PythonMod,\n\u001b[32m     88\u001b[39m     TruncToInt,\n\u001b[32m     89\u001b[39m )\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumbers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m int_oo\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sympy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprinters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CppPrinter, PythonPrinter\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\utils\\_sympy\\functions.py:597\u001b[39m\n\u001b[32m    593\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mnegative shift count\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    594\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m FloorDiv(base, \u001b[32m2\u001b[39m**shift)\n\u001b[32m--> \u001b[39m\u001b[32m597\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mMinMaxBase\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mExpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLatticeOp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[misc]\u001b[39;49;00m\n\u001b[32m    598\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43moriginal_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43massumptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    599\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfrom\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01msympy\u001b[39;49;00m\u001b[34;43;01m.\u001b[39;49;00m\u001b[34;43;01mcore\u001b[39;49;00m\u001b[34;43;01m.\u001b[39;49;00m\u001b[34;43;01mparameters\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01mimport\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mglobal_parameters\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\torch\\utils\\_sympy\\functions.py:646\u001b[39m, in \u001b[36mMinMaxBase\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    640\u001b[39m     obj.unique_summations_symbols = unique_summations_symbols\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m    643\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_satisfy_unique_summations_symbols\u001b[39m(\n\u001b[32m    645\u001b[39m     \u001b[38;5;28mcls\u001b[39m, args\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m ) -> Optional[\u001b[38;5;28mset\u001b[39m[\u001b[43msympy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore\u001b[49m.symbol.Symbol]]:\n\u001b[32m    647\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    648\u001b[39m \u001b[33;03m    One common case in some models is building expressions of the form\u001b[39;00m\n\u001b[32m    649\u001b[39m \u001b[33;03m    max(max(max(a+b...), c+d), e+f) which is simplified to max(a+b, c+d, e+f, ...).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    671\u001b[39m \u001b[33;03m    property. Otherwise, it returns a new set of unique symbols.\u001b[39;00m\n\u001b[32m    672\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) != \u001b[32m2\u001b[39m:\n",
      "\u001b[31mAttributeError\u001b[39m: module 'sympy' has no attribute 'core'"
     ]
    }
   ],
   "source": [
    "#Let's train the LSTM on the generated data using a ADAM optimizer and MSE loss\n",
    "\n",
    "# Configuration \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = lstm_model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, eps=1e-6) #automatic learning rate adjustment\n",
    "\n",
    "num_epochs = 50 #50 iterations\n",
    "\n",
    "# Historique des pertes\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(f\"Entraînement sur {device}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # === TRAINING ===\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # === VALIDATION ===\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Affichage\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"✓ Entraînement terminé\")\n",
    "\n",
    "# Visualisation of loss \n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.title('Training History')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Let's save the trained model\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_loss': train_losses[-1],\n",
    "    'val_loss': val_losses[-1],\n",
    "}, 'lstm_varfima_model.pth')\n",
    "\n",
    "print(\"✓ Modèle sauvegardé: lstm_varfima_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2405fe1a",
   "metadata": {},
   "source": [
    "#### Let's verify the long memory property in the LSTM trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "523d901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape des sorties LSTM: (52388, 50, 64)\n",
      "Shape pour analyse: (64, 2619400)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Charger le modèle entrainé \n",
    "model = LSTMPredictor(\n",
    "    input_size=k,           \n",
    "    hidden_size=64,        \n",
    "    dropout=0.2,           \n",
    "    forecast_horizon=1) \n",
    "model.load_state_dict(torch.load('lstm_varfima_model.pth')[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# Extraire les sorties LSTM pour tous les batchs\n",
    "def extract_lstm_outputs_from_loader(model, train_loader):\n",
    "    \"\"\"\n",
    "    Extrait les sorties du LSTM pour toutes les données du train_loader\n",
    "    \"\"\"\n",
    "    all_lstm_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # batch_X shape: (batch_size, seq_len, input_size)\n",
    "            h0 = torch.zeros(model.num_layers, batch_X.size(0), model.hidden_size)\n",
    "            c0 = torch.zeros(model.num_layers, batch_X.size(0), model.hidden_size)\n",
    "            \n",
    "            # Obtenir les sorties LSTM\n",
    "            lstm_out, _ = model.lstm(batch_X, (h0, c0))\n",
    "            \n",
    "            # lstm_out shape: (batch_size, seq_len, hidden_size)\n",
    "            all_lstm_outputs.append(lstm_out.numpy())\n",
    "    \n",
    "    # Concaténer tous les batchs\n",
    "    all_outputs = np.concatenate(all_lstm_outputs, axis=0)  # (total_samples, seq_len, hidden_size)\n",
    "    \n",
    "    return all_outputs\n",
    "\n",
    "# Extraire les sorties\n",
    "lstm_outputs = extract_lstm_outputs_from_loader(model, train_loader)\n",
    "print(f\"Shape des sorties LSTM: {lstm_outputs.shape}\")  # (n_samples, seq_len, hidden_size)\n",
    "\n",
    "#  Créer une séquence temporelle à partir des sorties: Prendre toutes les séquences et les concaténer temporellement\n",
    "# Chaque neurone caché devient une longue série temporelle\n",
    "seq_concatenated = lstm_outputs.transpose(2, 0, 1).reshape(lstm_outputs.shape[2], -1)\n",
    "print(f\"Shape pour analyse: {seq_concatenated.shape}\")  # (hidden_size, n_samples * seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e9575",
   "metadata": {},
   "source": [
    "We're using an already coded function \"compute_total_memory\" that compute the long memory parameter d, based on a temporal sequence and by using a GSE statistics. Then, this function conducts a test on the long memory on the computed statistics and returns the parameter d estimated and its p-value.\n",
    "\n",
    "More specificially the function is testing H0: d= 0 vs H0: d>0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff590c0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean() received an invalid combination of arguments - got (axis=NoneType, dtype=NoneType, out=NoneType, ), but expected one of:\n * (*, torch.dtype dtype = None)\n * (tuple of ints dim, bool keepdim = False, *, torch.dtype dtype = None)\n * (tuple of names dim, bool keepdim = False, *, torch.dtype dtype = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01md_test\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_total_memory\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tot_mem, asy_var, p_val = \u001b[43mcompute_total_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFD_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Analyse de mémoire longue ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParamètre d moyen: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtot_mem\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\Test Kelly\\d_test.py:84\u001b[39m, in \u001b[36mcompute_total_memory\u001b[39m\u001b[34m(seq)\u001b[39m\n\u001b[32m     81\u001b[39m k, T = seq.shape\n\u001b[32m     82\u001b[39m m = \u001b[38;5;28mint\u001b[39m(np.sqrt(T))\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m GSE = \u001b[43mmulti_GSE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m GSE[\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mL-BFGS-B optimization failed\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\Test Kelly\\d_test.py:73\u001b[39m, in \u001b[36mmulti_GSE\u001b[39m\u001b[34m(seq, m, options, method)\u001b[39m\n\u001b[32m     71\u001b[39m init_d = np.zeros(k)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     init_d[i] = \u001b[43md_lw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# initalize at vector of univariate estimates\u001b[39;00m\n\u001b[32m     75\u001b[39m opti = opt.minimize(R_d, init_d, args=(lam, pdg), method=method, bounds=[(-\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m)] * k, jac=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     76\u001b[39m                     options=options)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m opti\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\Test Kelly\\d_test.py:30\u001b[39m, in \u001b[36md_lw\u001b[39m\u001b[34m(seq, m)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34md_lw\u001b[39m(seq,m):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     lam,pdg = mypdg(seq-\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     31\u001b[39m     opti = opt.minimize(K_fn,\u001b[32m0\u001b[39m,args = (lam[\u001b[32m1\u001b[39m:m],pdg[\u001b[32m1\u001b[39m:m]),jac=K_grad,method=\u001b[33m'\u001b[39m\u001b[33mL-BFGS-B\u001b[39m\u001b[33m'\u001b[39m,bounds=[(-\u001b[32m0.5\u001b[39m,\u001b[32m0.5\u001b[39m)])\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m opti\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kelly\\Desktop\\ENSAE_3A\\Advanced_ML_project\\LSTM-long-memory\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3858\u001b[39m, in \u001b[36mmean\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m   3856\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   3857\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3858\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3860\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[32m   3861\u001b[39m                       out=out, **kwargs)\n",
      "\u001b[31mTypeError\u001b[39m: mean() received an invalid combination of arguments - got (axis=NoneType, dtype=NoneType, out=NoneType, ), but expected one of:\n * (*, torch.dtype dtype = None)\n * (tuple of ints dim, bool keepdim = False, *, torch.dtype dtype = None)\n * (tuple of names dim, bool keepdim = False, *, torch.dtype dtype = None)\n"
     ]
    }
   ],
   "source": [
    "from d_test import compute_total_memory\n",
    "\n",
    "#test mémoire longue sur séquence originale\n",
    "tot_mem, asy_var, p_val = compute_total_memory(FD_seq)\n",
    "\n",
    "print(f\"\\n=== Analyse de mémoire longue ===\")\n",
    "print(f\"Paramètre d moyen: {tot_mem:.4f}\")\n",
    "print(f\"Variance asymptotique: {asy_var:.6f}\")\n",
    "print(f\"P-value: {p_val:.4f}\")\n",
    "print(f\"Mémoire longue significative: {'OUI' if p_val < 0.05 else 'NON'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0501e572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Analyse de mémoire longue ===\n",
      "Paramètre d moyen: 0.0021\n",
      "Variance asymptotique: 0.003906\n",
      "P-value: 0.4867\n",
      "Mémoire longue significative: NON\n"
     ]
    }
   ],
   "source": [
    "from d_test import compute_total_memory\n",
    "\n",
    "# Test sur le paramètre de mémoire longue d via la statistique GSE\n",
    "tot_mem, asy_var, p_val = compute_total_memory(seq_concatenated)\n",
    "\n",
    "print(f\"\\n=== Analyse de mémoire longue ===\")\n",
    "print(f\"Paramètre d moyen: {tot_mem:.4f}\")\n",
    "print(f\"Variance asymptotique: {asy_var:.6f}\")\n",
    "print(f\"P-value: {p_val:.4f}\")\n",
    "print(f\"Mémoire longue significative: {'OUI' if p_val < 0.05 else 'NON'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f5eb7",
   "metadata": {},
   "source": [
    "# Test 2 (complementary) Long memory transformation of White Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd1fe7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generation of a WN sequence of length T and size k=200\n",
    "#training of the LSTM model on this WN sequence\n",
    "#computing the statistics of the learned LSTM\n",
    "#checking if d=0 (short memory) or d>0 (long memory)\n",
    "#OK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
